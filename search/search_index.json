{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Django application to produce/consume events from Kafka","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install django-streams\n</code></pre> <p>or with poetry</p> <pre><code>poetry add django-streams\n</code></pre> <p>and add it to <code>INSTALLED_APPS</code>:</p> <pre><code>INSTALLED_APPS = [\n    ...\n    \"django_streams\",\n    ...\n    \"my_streams_app\",\n    # etc...\n]\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>https://kpn.github.io/django-streams/</p>"},{"location":"#usage","title":"Usage","text":"<p>create the <code>engine</code>:</p> <pre><code># my_streams_app/engine.py\nfrom django_streams import create_engine\n\nfrom kstreams.backends import Kafka\n\n\nstream_engine = create_engine(\n    title=\"test-engine\",\n    backend=Kafka(),\n)\n</code></pre> <p>Note</p> <p>To configure the backend follow the kstreams backend documentation</p>"},{"location":"#consuming-events","title":"Consuming events","text":"<p>Define your streams:</p> <pre><code># my_streams_app/streams.py\nfrom kstreams import ConsumerRecord\nfrom .engine import stream_engine\n\n\n@stream_engine.stream(\"dev-kpn-des--hello-kpn\", group_id=\"django-streams-principal-group-id\")  # your consumer\nasync def consumer_task(cr: ConsumerRecord):\n    async for cr in stream:\n        logger.info(f\"Event consumed: headers: {cr.headers}, value: {cr.value}\")\n</code></pre> <p>and then in your <code>apps.py</code> you must import the <code>python module</code> or your <code>coroutines</code></p> <pre><code># my_streams_app/apps.py\nfrom django.apps import AppConfig\n\n\nclass StreamingAppConfig(AppConfig):\n    name = \"streaming_app\"\n\n    def ready(self):\n        from . import streams  # import the streams module\n</code></pre> <p>Now you can run the worker:</p> <pre><code>python manage.py worker\n</code></pre>"},{"location":"#producing-events","title":"Producing events","text":"<p>Producing events can be <code>sync</code> or <code>async</code>. If you are in a <code>sync</code> context you must use <code>stream_engine.sync_send</code>, otherwise stream_engine.send. For both cases a <code>RecordMetadata</code> is returned.</p> <pre><code># streaming_app/views.py\nfrom django.http import HttpResponse\nfrom django.views.generic import View\n\nfrom de.core.conf import settings\nfrom .engine import stream_engine\n\n\nclass HelloWorldView(View):\n\n    def get(self, request, *args, **kwargs):\n        topic = f\"{settings.KAFKA_TOPIC_PREFIX}hello-kpn\"\n\n        record_metadata = stream_engine.sync_send(\n            topic,\n            value=b\"hello world\",\n            key=\"hello\",\n            partition=None,\n            timestamp_ms=None,\n            headers=None,\n        )\n\n        return HttpResponse(f\"Event metadata: {record_metadata}\")\n</code></pre>"},{"location":"#benchmark","title":"Benchmark","text":"<p>Producer:</p> Total produced events Time (seconds) 1 0.004278898239135742 10 0.030963897705078125 100 0.07049298286437988 1000 0.6609988212585449 10000 6.501222133636475"},{"location":"#running-tests","title":"Running tests","text":"<pre><code>./scrtips/test\n</code></pre>"},{"location":"#code-formating","title":"Code formating","text":"<pre><code>./scrtips/format\n</code></pre>"},{"location":"kubernetes_deployment/","title":"Kubernetes deployment","text":"<p>We advise you to have a separate <code>deployment</code> for scale <code>up/down</code> your <code>kafka consumers</code> independently of your <code>application</code>. This is useful because you might need more/less <code>kafka consumers</code> (replicas) to handle events and the number of replicas most of the time is different from the <code>application replicas</code>.</p> <pre><code># Source: django-streaming-example/templates/streaming-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: django-streaming-example-worker\n  labels:\n    app.kubernetes.io/name: django-streaming-example\n    app.kubernetes.io/instance: django-streaming-example-preview-latest\n    app.kubernetes.io/version: latest\n    app.kubernetes.io/component: backend\n    app.kubernetes.io/managed-by: Helm\n    app: django-streaming-example\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 10%\n  selector:\n    matchLabels:\n      app: django-streaming-example\n  template:\n    metadata:\n      labels:\n        app: django-streaming-example\n      annotations:\n        co.elastic.logs/enabled: \"true\"\n    spec:\n      serviceAccountName: django-streaming-example-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - django-streaming-example\n              topologyKey: kubernetes.io/hostname\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - django-streaming-example\n              topologyKey: failure-domain.beta.kubernetes.io/zone\n      containers:\n      - name: streaming\n        image: django-streams:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        args: [\"make\", \"worker\"]\n        resources:\n          limits:\n            cpu: 600m\n            memory: 256Mi\n          requests:\n            cpu: 300m\n            memory: 128Mi\n        envFrom:\n          - configMapRef:\n              name: django-streaming-example-config\n        volumeMounts:\n        - mountPath: /secrets\n          name: secrets-folder\n      volumes:\n      - name: secrets-folder\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Mi\n</code></pre>"},{"location":"producer/","title":"Producing events","text":"<p>Produce events in your <code>django</code> applications is quite straightforward. It can be done in an <code>async</code> or <code>sync</code> context.</p> <ul> <li>In a <code>sync</code> context for example in a <code>django view</code> or a <code>background task</code> you must use the <code>sync_send</code> method.</li> <li>In an <code>async</code> context, for example in a <code>kafka consumer</code> you must use the <code>send</code> method.</li> </ul>"},{"location":"producer/#producing-in-a-sync-context","title":"Producing in a sync context","text":"<p>The following example shows the simplest and probably common use case: producing an event in a <code>django view</code>:</p> <pre><code># streaming_app/views.py\nfrom django.http import HttpResponse\nfrom django.views.generic import View\n\nfrom de.core.conf import settings\nfrom .engine import stream_engine\n\n\nclass HelloWorldView(View):\n    def get(self, request, *args, **kwargs):\n        topic = f\"{settings.KAFKA_TOPIC_PREFIX}hello-kpn\"\n\n        record_metadata = stream_engine.sync_send(\n            topic,\n            value=b\"hello world!!!\",\n            key=\"hello\",\n            partition=None,\n            timestamp_ms=None,\n            headers=None,\n        )\n\n        return HttpResponse(f\"Event metadata: {record_metadata}\")\n</code></pre> <p>Note</p> <p>The engine must be created in order to use the <code>sync_send</code> function</p> <p>Note</p> <p>Any extra metadata for example <code>Content Type</code> can be specified using the header <code>content-type</code></p> <p>Note</p> <p>The returned value after producing is <code>RecordMetadata</code></p>"},{"location":"producer/#producing-in-an-async-context","title":"Producing in an async context","text":"<p>Producing events in an <code>async</code> context, for example inside a coroutine must be done using <code>await engine.send(...)</code></p> <pre><code>from kstreams import ConsumerRecord\n\nfrom .engine import stream_engine\n\n\nhello_world_topic = \"dev-kpn-des--hello-kpn\"\nhello_world_two_topic = \"dev-kpn-des--hello-kpn-2\"\n\n@stream_engine.stream(hello_world_topic, group_id=\"django-streaming-example-group-id\")\nasync def consumer_task(cr: ConsumerRecord):\n    logger.info(f\"Event consumed: headers: {headers}, payload: {payload}\")\n\n    await stream_engine.send(\n        hello_world_two_topic,\n        value=b\"hello world!!!,\n        key=\"hello\",\n    )\n</code></pre> <p>!!! note:     Do not use the function <code>stream_engine.sync_send(...)</code> inside a coroutine because it is blocking!!</p>"},{"location":"test_client/","title":"Test Client","text":"<p>To test your <code>streams</code> or perform <code>e2e</code> tests you can make use of the <code>test_utils.TestStreamClient</code>. The <code>TestStreamClient</code> you can send events so you won't need a <code>producer</code></p> <p>Let's assume that you have the following code example:</p> <pre><code># streams.py\nfrom kstreams import ConsumerRecord\n\nfrom .engine import stream_engine\n\n\ntopic = \"dev-kpn-des--kstreams\"\n\ndef save_to_db(value):\n    # Store the value in your Database\n    ...\n\n\n@stream_engine.stream(topic, group_id=\"example-group\")\nasync def consume(cr: ConsumerRecord):\n    print(f\"Event consumed: headers: {cr.headers}, payload: {cr.value}\")\n    save_to_db(value)\n</code></pre> <p>In order to test it, you could mock the kafka <code>Consumer</code> and <code>Producer</code> or use the <code>TestStreamClient</code></p> <pre><code># test_stream.py\nimport pytest\nfrom django_streams.test_utils import TestStreamClient\n\nfrom .engine import stream_engine\n\n\n@pytest.mark.asyncio\nasync def test_streams_consume_events():\n    topic = \"dev-kpn-des--kstreams\"  # Use the same topic as the stream\n    event = b'{\"message\": \"Hello world!\"}'\n\n    with patch(\"example.on_consume\") as save_to_db:\n        async with TestStreamClient(stream_engine=stream_engine) as test_client:\n            metadata = await test_client.send(topic, value=event, key=\"1\")  # send the event with the test client\n            current_offset = metadata.offset\n            assert metadata.topic == topic\n\n            # send another event and check that the offset was incremented\n            metadata = await test_client.send(topic, value=b'{\"message\": \"Hello world!\"}', key=\"1\")\n            assert metadata.offset == current_offset + 1\n\n    # check that the event was consumed\n    assert save_to_db.called\n</code></pre>"},{"location":"test_client/#sync-producer-only","title":"Sync Producer only","text":"<p>In some scenarios, your application will only produce events in a <code>synchronous</code> way and other application/s will consume it, but you want to make sure that the event was procuced in a proper way and the <code>topic</code> contains that <code>event</code>.</p> <pre><code># producer_example.py\nfrom .engine import stream_engine\n\n\ntopic = \"dev-kpn-des--hello-world\"\n\n\ndef produce(event):\n    # function to produce events. This function can be\n    # a django view where an event is produced\n    stream_engine.sync_send(topic, value=event, key=\"1\")\n</code></pre> <p>Then you could have a test_producer_example.py file to test the code:</p> <pre><code>from producer_example import topic, produce\n\n\n@pytest.mark.asyncio\nasync def test_e2e_with_sync_producer_no_stream(stream_engine: StreamEngine):\n    event = b'{\"message\": \"Hello world!\"}'\n\n    async with TestStreamClient(stream_engine=stream_engine) as client:\n        # produce 2 events\n        produce(event)\n        produce(event)\n\n        # check that the event was placed in a topic in a proper way\n        consumer_record = await client.get_event(topic_name=topic)\n        assert consumer_record.value == event\n\n        # check that the event was placed in a topic in a proper way\n        consumer_record = await client.get_event(topic_name=topic)\n        assert consumer_record.value == event\n</code></pre>"},{"location":"using_orm/","title":"Using the Django ORM","text":"<p>Django is a <code>synchronous</code> framework but as it was explained before, we can run <code>coroutines</code>. The coroutines can <code>await</code> other <code>coroutines</code> and also call <code>sync</code> functions but they CAN NOT call the django ORM directly. If you have to call the ORM, you MUST use the <code>sync_to_async</code> adapter, otherwise <code>django</code> will raise an error</p> <pre><code>import logging\nfrom asgiref.sync import sync_to_async\nfrom kstreams import ConsumerRecord\n\nfrom streaming.models import HelloWorld  # Your Model\nfrom streaming.engine import stream_engine\n\nlogger = logging.getLogger(__name__)\n\n\n@sync_to_async\ndef get_object():\n    return HelloWorld.objects.get_or_create(pk=1)\n\n\n@sync_to_async\ndef increase(my_object):\n    my_object.total += 1\n    my_object.save()\n\n    logger.info(f\"Total increased to {my_object.total}\")\n\n\n\n@stream_engine.stream(\"dev-des--hello-kpn\", group_id=\"my-group-id\")\nasync def consumer_task(cr: ConsumerRecord)::\n    logger.info(f\"Event consumed: headers: {cr.headers}, payload: {cr.value}\")\n\n    my_object, _ = await get_object()\n    await increase(my_object)\n</code></pre> <p>Note</p> <p>The package <code>asgiref</code> is required in order to use <code>sync_to_async</code> or <code>async_to_sync</code>. Use <code>poetry add asgiref</code></p>"},{"location":"worker/","title":"How the worker runs?","text":"<p><code>django-streams</code> has a custom command to run the worker that will take of running the <code>streams</code> and manage the <code>gracefully shutdown</code>.</p> <pre><code>python manage.py worker\n</code></pre> <p>The custom <code>django command</code> does two things:</p> <ol> <li>Subscribe to <code>signal.SIGINT</code> and <code>signal.SIGTERM</code> to stop the worker</li> <li>Start the <code>engine</code></li> </ol> <pre><code># django_streams.management.commands.worker.py\nimport logging\nimport signal\n\nfrom django.core.management.base import BaseCommand\n\nfrom django_streams.factories import create_engine\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(BaseCommand):\n    help = \"Start kafka consumers\"\n\n    def handle(self, *args, **options):\n        # StreamEngine is a Singlenton, so it will return the same instance\n        # as the user has defined in the custom django app.\n        engine = create_engine()\n        logger.info(f\"Starting Engine with streams {engine._streams}\")\n\n        # Listening signals from main Thread\n        signal.signal(signal.SIGINT, engine.sync_stop)  # IMPORTANT\n        signal.signal(signal.SIGTERM, engine.sync_stop) # IMPORTANT\n\n        # start worker\n        engine.sync_start()\n</code></pre>"},{"location":"worker/#signals","title":"Signals","text":"<p>When you write a custom command in <code>django</code> you are in a <code>sync</code> context but <code>coroutines</code> must run in an <code>async</code> context.</p> <p>When the <code>command/program</code> finishes, the <code>main thread</code> finishes but not the <code>secondary thread</code>, unless that the worker is subscribed to the signals <code>signal.SIGINT</code> and <code>signal.SIGTERM</code>. Subscribing to the signals will guarantee that:</p> <ol> <li>The <code>kafka consumers</code> are stopped in a proper way.</li> <li>Gracefully shutdown.</li> <li>If you are using <code>kubernetes</code>, the pod will be terminated faster.</li> </ol> <p>So, the <code>worker</code> have to subscribe to the signals:</p> <pre><code>def handle(self, *args, **options):\n    # Listening signals from main Thread\n    signal.signal(signal.SIGINT, engine.sync_stop)  # IMPORTANT\n    signal.signal(signal.SIGTERM, engine.sync_stop) # IMPORTANT\n\n    # start the engine\n    engine.sync_start()\n</code></pre>"}]}